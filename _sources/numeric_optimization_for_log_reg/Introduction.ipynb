{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f9b04c2",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "For binary classification issues, logistic regression is a potent statistical technique. Logistic regression predicts the likelihood that an instance falls into a certain category as opposed to linear regression, which forecasts continuous values. Because of this, it's especially helpful in situations when the target variable is binary, meaning it can have two alternative values.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The link between one or more independent variables ($ X $) and the likelihood of a specific result ($ Y $) is modeled using logistic regression. The foundation of logistic regression is the logistic function, sometimes referred to as the sigmoid function. The definition of the sigmoid function is:\n",
    "\n",
    "$$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "When the linear combination of the input characteristics and model parameters is represented by $ z $:\n",
    "\n",
    "$$ z = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_n X_n $$\n",
    "\n",
    "The coefficients to be learned from the training set are $\\beta_0, \\beta_1, \\ldots, \\beta_n $.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Probability and Odds\n",
    "\n",
    "The probability ($ P $) that an instance belongs to the positive class is modeled by logistic regression. The odds of a favorable outcome are represented by the odds ratio ($ \\frac{P}{1-P} $).\n",
    "\n",
    "### Log-Odds (Logit)\n",
    "\n",
    "The log-odds or logit function is used to map the odds to a continuous range:\n",
    "\n",
    "$$ \\ln\\left(\\frac{P}{1-P}\\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_n X_n $$\n",
    "\n",
    "### Decision Boundary\n",
    "\n",
    "In logistic regression, an important concept is the decision border. The hypersurface is the one that divides the instances into several classes. The decision boundary for a binary classification issue is given by the equation $ z = 0 $, which is equivalent to $\\sigma(z) = 0.5 $.\n",
    "\n",
    "## Training Logistic Regression\n",
    "\n",
    "Finding the ideal values for the coefficients ($\\beta$) that maximize the likelihood of the observed data is the first step in training a logistic regression model. Usually, numerical optimization techniques like Newton's method or gradient descent are used for this.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "Logistic regression can be easily implemented in Python using libraries like scikit-learn. Here's a simple example:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assume X_train, y_train, X_test, y_test are your training and testing data\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "```\n",
    "\n",
    "This example demonstrates the basic steps of training a logistic regression model and evaluating its accuracy using scikit-learn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
