{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d763c3c9",
   "metadata": {},
   "source": [
    "# Cost Function\n",
    "\n",
    "The cost function in logistic regression is an essential tool for calculating the discrepancy between the actual labels and the predicted probabilities. Finding the best values for the model parameters involves minimizing this cost function, which is the aim of logistic regression.\n",
    "\n",
    "## Sigmoid Function\n",
    "\n",
    "An essential part of logistic regression is the logistic or sigmoid function. It has the following definition:\n",
    "\n",
    "$$ \\sigma(z) = \\frac{1}{1 + e^{-z}} $$\n",
    "\n",
    "where $ z $ represents the linear combination of the model's parameters and input features:\n",
    "\n",
    "$$ z = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_n X_n $$\n",
    "\n",
    "The coefficients that need to be learned in this case are $\\beta_0, \\beta_1, \\ldots, \\beta_n $.\n",
    "\n",
    "## Probability and Odds\n",
    "\n",
    "The probability ($ P $) that an instance belongs to the positive class is predicted by the logistic regression model. The odds of a positive outcome are represented by the odds ratio ($ \\frac{P}{1-P} $).\n",
    "\n",
    "## Log-Odds (Logit)\n",
    "\n",
    "The log-odds or logit function is used to map the odds to a continuous range:\n",
    "\n",
    "$$ \\ln\\left(\\frac{P}{1-P}\\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_n X_n $$\n",
    "\n",
    "## Binary Cross-Entropy Loss\n",
    "\n",
    "The binary cross-entropy loss is a common way to represent the cost function in logistic regression. It is provided by: for a single training example.\n",
    "\n",
    "$$ J(\\theta) = -\\left[y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y})\\right] $$\n",
    "\n",
    "where $ \\hat{y} $ is the expected probability and $ y $ is the actual label (0 or 1).\n",
    "\n",
    "## Cost Function for the Entire Dataset\n",
    "\n",
    "The overall cost function for the full dataset with $ m $ training examples is the mean of the individual costs:\n",
    "\n",
    "$$ J(\\theta) = -\\frac{1}{m}\\sum_{i=1}^{m}\\left[ y^{(i)}\\log(\\hat{y}^{(i)}) + (1-y^{(i)})\\log(1-\\hat{y}^{(i)}) \\right] $$\n",
    "\n",
    "## Minimizing the Cost Function\n",
    "\n",
    "Finding the parameter values ($ \\beta $) that minimize the cost function is the aim of the training process. Numerical optimization techniques like gradient descent, Newton's method, or stochastic gradient descent are commonly used to accomplish this.\n",
    "\n",
    "## Definition\n",
    "\n",
    "The cost function for logistic regression is defined as:\n",
    "\n",
    "$$ J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)}))] $$\n",
    "\n",
    "Here, $ J(\\theta) $ is the cost function, $ m $ is the number of training examples, $ y^{(i)} $ is the actual output for the $ i^{th} $ example, $ h_\\theta(x^{(i)}) $ is the predicted output, and $ \\theta $ represents the parameters of the model.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_cost(theta, X, y):\n",
    "    m = len(y)\n",
    "    h = sigmoid(np.dot(X, theta))\n",
    "    cost = -1/m * (np.dot(y, np.log(h)) + np.dot((1 - y), np.log(1 - h)))\n",
    "    return cost\n",
    "\n",
    "# Example usage:\n",
    "# Assuming X is your feature matrix and y is the target variable\n",
    "# Add a column of ones to X for the bias term\n",
    "X = np.column_stack((np.ones(m), X))\n",
    "theta = np.zeros(X.shape[1])\n",
    "cost = compute_cost(theta, X, y)\n",
    "\n",
    "print(\"Initial cost:\", cost)\n",
    "```\n",
    "\n",
    "In this example, sigmoid is the sigmoid activation function, and compute_cost calculates the logistic regression cost using the given formula. This cost is what the optimization algorithm e.g., gradient descent or Newton's method aims to minimize during the training process by adjusting the parameters $ \\theta $.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
